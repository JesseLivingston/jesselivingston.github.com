<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Java | 小重山]]></title>
  <link href="http://jesselivingston.github.io/blog/categories/java/atom.xml" rel="self"/>
  <link href="http://jesselivingston.github.io/"/>
  <updated>2015-08-21T21:33:51+08:00</updated>
  <id>http://jesselivingston.github.io/</id>
  <author>
    <name><![CDATA[Livingston Chen]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Hadoop 教程1]]></title>
    <link href="http://jesselivingston.github.io/blog/2015/07/17/hadoop-tutorial-1/"/>
    <updated>2015-07-17T01:30:36+08:00</updated>
    <id>http://jesselivingston.github.io/blog/2015/07/17/hadoop-tutorial-1</id>
    <content type="html"><![CDATA[<p>环境： Mac, JDK8, Maven3</p>

<ol>
<li>下载二进制版，这种 java 代码的玩意儿就同必要从源码去编译了吧，怎么方便怎么来</li>
<li>先在单机上跑一下试试</li>
<li><p>在下载下来的 hadoop 目录里，创建个 input 目录</p>

<p> <code>$mkdir input
</code></p></li>
<li><p>把 etc/hadoop 目录下的 xml 文件复制进去，后面使用</p>

<p> <code>$cp etc/hadoop/*.xml input
</code></p></li>
<li><p>统计一下 input 目录里的文件内容匹配 &lsquo;dfs[a-z.]+&rsquo; 的地方，结果放到 output 目录里</p>

<p> <code>$bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar grep input output 'dfs[a-z.]+'
</code></p></li>
<li><p>查看一下 output 的内容</p>

<p> <code>$cat output/*
</code></p></li>
<li><p>以伪分布式的方式玩玩，首先修改 etc/hadoop/core-site.xml:</p>

<p> <code>&lt;configuration&gt;
     &lt;property&gt;
         &lt;name&gt;fs.defaultFS&lt;/name&gt;
         &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
     &lt;/property&gt;
 &lt;/configuration&gt;
</code></p></li>
<li><p>修改 etc/hadoop/hdfs-site.xml:</p>

<p> <code>&lt;configuration&gt;
     &lt;property&gt;
         &lt;name&gt;dfs.replication&lt;/name&gt;
         &lt;value&gt;1&lt;/value&gt;
     &lt;/property&gt;
 &lt;/configuration&gt;
</code></p></li>
<li><p>为了能够无密钥 ssh 登录 localhost，需要生成一对 dsa 算法的加密密钥，并把公钥放到 authorized_keys 里去</p>

<p> <code>$ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa</code></p>

<p> <code>$cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys</code></p>

<p> <code>$export HADOOP_PREFIX=/usr/local/hadoop</code></p>

<p> 不要按官网上教程抄，HADOOP_PREFIX 中的下划线不需要转义</p></li>
<li><p>格式化文件系统，虽然我目前也不是太了解 HDFS</p>

<p><code>$bin/hdfs namenode -format</code></p></li>
<li><p>以后台进程的形式启动 NameNode, DataNode:</p>

<p><code>$sbin/start-dfs.sh</code>
然后就可以打开 <a href="http://localhost:50070/">localhost:50070</a> 查看 NameNode 了</p></li>
<li><p>建立 MapReduce jobs 执行必需的目录，我的用户名就是 admin:</p>

<p><code>$bin/hdfs dfs -mkdir /user</code></p>

<p><code>$bin/hdfs dfs -mkdir /user/admin</code></p></li>
<li><p>把etc/hadoop 里的文件复制到分布式文件系统中</p>

<p><code>$bin/hdfs dfs -put etc/hadoop input</code></p></li>
<li><p>执行 example:</p>

<p><code>$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar grep input output 'dfs[a-z.]+'</code></p></li>
<li><p>有两种方式查看上面的结果， 一种是先把分布式文件系统中的文件复制到本地文件系统中来再查看:</p>

<p><code>$ bin/hdfs dfs -get output output</code></p>

<p><code>$ cat output/*</code></p>

<p>一种是直接查看分布式文件系统上的内容:</p>

<p><code>$ bin/hdfs dfs -cat output/*</code></p></li>
<li><p>与 11 配套的， 关闭后台的 NameNode, DataNode:</p>

<p><code>$ sbin/stop-dfs.sh</code></p></li>
</ol>


<p>好在以前接触过 solr，这种分布式的东西了解一点，后面再慢慢把这里面每一行都确切地搞清楚做了什么事。</p>
]]></content>
  </entry>
  
</feed>
